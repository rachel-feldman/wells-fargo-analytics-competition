{"name":"Wells Fargo Analytics Competition","tagline":"","body":"<h1>A Plan for Action for Social Media Response</h1>\r\n<h3>BY RACHEL FELDMAN, JENIFFER SOTO-PEREZ, AND CASEY SALVADOR</h3>\r\n\r\n<h2> The Competition</h2>\r\n<p>> > Dialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data. </p>\r\n\r\n<h2>1\tPREPARING THE DATA</h2>\r\n<p>In order to pull useful information from the data given, we but the dataset in R studio.  We mined the tweets and posts through R Studio, getting rid of blank spaces, messages that did not tag a particular bank.  Additionally, we spell checked the information. </p> \r\n\r\n<h2>2\tAPPROACH AND METHODOLOGY</h2>\r\n<p>Deliverable A - Describe your Approach and Methodology. Include a visual representation of your analytic process flow. \r\nOnce we prepared the tweets and posts for analysis, we used an agile methodology so we could test small amounts of data at a time to quicken the process, that would be representative of things we could get when testing the entire data set.  \r\nAn agile methodology was chosen so the many questions could be answered and so methods could be repeated and compared for each bank. Through a process, we tested 1000 random samples of the entire data set and then 1000 random samples of only bank A to compare and contrast differences.\r\nBelow, find a graphical representation of the analytic and developmental approach taken.  It will show first cleaning the data where the team got rid of any additional room that was taking up space.  The team then tested the data overall to find positive and negative sentiment words as well as high-frequency words.  This gave us the framework to look at the data as a whole before looking into the context of each bank.  This step also allowed for the creation of two separate files for negative and positive posts.  \r\nThe team indexed each bank to then replicate the tests: positive and negative as well as word frequency. \r\nThe results were then tested against the results as a whole.</p>\r\n\r\n![Method](http://imgur.com/XI5Eh29)\r\n<h2>3\tSOCIAL CONVERSATION STARTERS</h2>\r\n<p>Deliverable B - Discuss the data and its relationship to social conversation drivers. \r\nTo find the social conversation drivers, we started to index the data.  First, we did this by positive vs. negative.  We then asked the questions of what started the conversation and what variables, from the given data, connect.\r\nQuestions that connect to this were is there a correlation between positive/ negative remarks on different days of the week or times of the month?  Is there a different sort of sentiment on twitter compared to facebook?\r\nIf given additional information, more questions that could be answered are is location a factor in and of these posts? Can we geo locate, through social media, a branch that is doing really well vs poorly? Does location drive a certain feeling toward one bank or another?  Another group of questions that could be answered with additional information is time and if Time is a factor for overall sentiment, topic, or substance.  Finally, an interesting factor into all of these questions would be looking at the ages of the different posters and seeing if there is any correction between age, sentiment, topic, substance, location, and time of day.\r\nAfter answering these questions for a specific bank and finding numbers, comparing these to the overall internet averages would be beneficial to see where the banking world falls in social conversation compared to other industries. \r\n</p>\r\n\r\n<h2> CODE<\\h2>\r\n\r\n\r\n```R\r\n# Load data set ('df.Rda')\r\nload(\"df.Rda\")\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Grab just the texts, so you can load them in the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \r\n                                     \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n# To test on 10000 samples using df.10000\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load in corpus form using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))   \r\n\r\n# Perform pre-processing\r\ndocs <- tm_map(docs, PlainTextDocument)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removeWords,c(\"Name\",\"and\",\"for\")) \r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind=\"SMART\"))\r\n\r\nsave.image('docs.preprocessed.Rda')\r\nload('docs.preprocessed.Rda')\r\n\r\n# Documents containing each bank\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\ndf$BankID = vector(mode=\"numeric\", length = nrow(df))\r\ndf$BankID[bankA.idx] = \"BankA\"\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\nsummary(docs)\r\n\r\n## Repeat these processes for every bank\r\n## Create document term matrix\r\ndtm <- DocumentTermMatrix(docs[bankA.idx])\r\n\r\n## Transpose this matrix\r\ntdm <- TermDocumentMatrix(docs[bankA.idx])\r\n\r\n## Remove sparse terms\r\ndtm = removeSparseTerms(dtm, 0.98)\r\n\r\n## Organize terms by frequency\r\nfindFreqTerms(dtm,50)\r\nfreq <- colSums(as.matrix(dtm))  \r\nord <- order(freq)   \r\nfreq[head(ord)]  \r\nfreq[tail(ord)]\r\n\r\nwf <- data.frame(word=names(freq), freq=freq)   \r\nhead(wf)\r\n\r\n## Plot word frequencies\r\nlibrary(ggplot2)   \r\np <- ggplot(subset(wf, freq>100), aes(word, freq))    \r\np <- p + geom_bar(stat=\"identity\")   \r\np <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np\r\n\r\n## To get a word cloud of the 100 most frequent words \r\nlibrary(wordcloud)\r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")   \r\nwordcloud(names(freq), freq, max.words=25, rot.per=0.2, colors=dark2)\r\n\r\n# Sentiment analysis\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n\r\n    # Clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    sentence = tolower(sentence)\r\n\r\n    # Split into words. You need the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # Sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n\r\n    # Compare words to positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n\r\n    # match() returns the position of the matched term or NA\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n\r\n    # TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n\r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n\r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\n# Very positive and negative\r\ndf.sentiment = df[bankA.idx,]\r\n\r\nscores = score.sentiment(df.sentiment$FullText, pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\npos.tweets = which(scores$very.pos == 1)\r\nneg.tweets = which(scores$very.neg == 1)\r\nwrite.csv(df.sentiment[pos.tweets,],file='pos.texts.csv')##creates positive\r\nwrite.csv(df.sentiment[neg.tweets,],file='neg.texts.csv')##creates negative\r\n\r\n# Creating a classifier for pos.texts\r\nload('df.Rda')\r\ndf$FullText = as.character(df$FullText)\r\npos.texts = read.csv('pos.texts.csv',header=T)\r\npos.texts$FullText = as.character(pos.texts$FullText)\r\ncolnames(pos.texts)\r\ndocs <- Corpus(DataframeSource(as.data.frame(pos.texts[,9])))   \r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\n\r\ntdm <- TermDocumentMatrix(docs)\r\n\r\nm = as.matrix(dtm)\r\n\r\ndf.classification = as.data.frame(m) #dataframe\r\ndf.classification$Relevant = pos.texts$Relevant\r\ndf.classification$Relevant\r\n\r\n# Grow a tree\r\nlibrary(rpart)\r\nfit<-rpart(Relevant ~ FullText + BankID, data = pos.texts, method = \"class\")\r\nprintcp(fit) # display the results \r\nplotcp(fit) # visualize cross-validation results \r\nsummary(fit) # detailed summary of splits\r\n\r\n# Creating a classifier for neg.texts\r\nload('df.Rda')\r\ndf$FullText = as.character(df$FullText)\r\nneg.texts = read.csv('neg.texts.csv',header=T)\r\nneg.texts$FullText = as.character(neg.texts.texts$FullText)\r\ncolnames(neg.texts)\r\ndocs <- Corpus(DataframeSource(as.data.frame(neg.texts[,9])))   \r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\n\r\ntdm <- TermDocumentMatrix(docs)\r\n\r\nm = as.matrix(dtm)\r\n\r\ndf.classification = as.data.frame(m) #dataframe\r\ndf.classification$Relevant = neg.texts$Relevant\r\ndf.classification$Relevant\r\ncolnames(neg.texts)\r\n\r\n# Grow a tree\r\nlibrary(rpart)\r\nfit<-rpart(Relevant ~ FullText + BankID, data = neg.texts, method = \"class\")\r\nprintcp(fit) # display the results \r\nplotcp(fit) # visualize cross-validation results \r\nsummary(fit) # detailed summary of splits\r\n\r\n## Cluster Dendrogram\r\n# Useless words\r\ndocs <- tm_map(docs, removeWords,c(\"Name\", \"and\",\"for\", \"name\", \"this\", \"are\",\"from\", \"just\", \"get\", \"ret_twit\", \"name_resp\", \"twit_hndl\", \"twit_handl:\", \"twit_hndl_banka\",\"ly/\")) \r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind=\"SMART\"))\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\ntdm <- TermDocumentMatrix(docs)\r\n\r\n# remove sparse terms\r\ntdm2 <- removeSparseTerms(tdm, sparse = 0.97)\r\nm2 <- as.matrix(tdm2)\r\n# cluster terms\r\ndistMatrix <- dist(scale(m2))\r\nfit <- hclust(distMatrix, method = \"ward.D\")\r\n\r\nplot(fit)\r\nrect.hclust(fit, k = 6)\r\n\r\n##############\r\n```\r\n<h2> 5\tTOPICS DISCUSSED</h2>\r\n<p>Looking at the positive and negative posts separately, the three main topics discussed were customer service, fees and ATM.\r\nCustomer Service\r\n-\tPositive tones: Customer service assisted user with information about fees\r\n-\tNegative tone: Users dissatisfied about fees and customer service\r\nTop 10 Topics discussed: Customer Service\r\n1.\tATM\r\n2.\tSecurity\r\n3.\tPrivacy\r\n4.\tApp\r\n5.\tAccount Access\r\n6.\tAccount Request\r\n7.\tCard\r\n8.\tCustomer Loss\r\n9.\tFees\r\n\r\nATM\r\n*Customer dislikes interacting with ATM due to lack of privacy, security, and customer service. \r\n*Card often goes unread at ATM’s and stores.\r\n\r\nCustomer Service \r\n*Customer is leaving bank because of rude bankers.\r\n*Customer Service was not helpful in explaining fees.\r\n*Customer dislikes ATM due to lack of customer service\r\n\r\nFees\r\n*Customer  leaving bank  because of excessive fees.\r\n*Customer has found more favorable conditions  at different bank .\r\n*Too many fees withdrawing money at ATMs \r\n\r\n</p>\r\n\r\n<h2>INSIGHTS</h2>\r\n\r\n<h3>Classification to find comments of substance. </h3>\r\n\r\n<p>We started creating a code that would classify the tweets and posts into spam or real.  We did this for the positive and negative posts separately to see the differences and similarities between what can be responded to, both on social media, as well as from a business stand-point.  In doing so, and testing on parts of the data, we created two decision trees that showed what kew words (aside from names, twitter handle, and the bank name)  \r\nOur suggestion would be to have the people responding to tweets/ posts putting if they are spam or not so the algorithm becomes more sophisticated.  In doing so, when looking at the tweets later on, the data will be better minned if there was an algorithm that took out nonrelevant or ‘spam’ tweets and posts.  </p>\r\n![](http://imgur.com/MVc6AGL)\r\nAccording to our tree diagram, this diagram displays the positive relevant words. This is significant because here we are able to plot the relevancy of positive sentiments as it shows overall bank's performance within the industry. When banks are acknowledge for their service they receive positive statements within a .5 relevancy.</p>\r\n\r\n<h3> Frequent Topics</h3>\r\n![](http://imgur.com/b55BLtt)\r\n\r\n![](http://imgur.com/e0zrX0w)\r\n\r\n<p>Regardless of sentiment value, the four banks were surveyed to find the most common topics for each, represented in a word cloud.\r\nBank A included service, account, money, check, help, customer service and phone.  These words show that most of the posts do have to do with the bank and the conversations are about aspects of the bank.  Also, it is of note that bankB is commonly mentioned so bankA and bankB may be close competitors.</p>\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}